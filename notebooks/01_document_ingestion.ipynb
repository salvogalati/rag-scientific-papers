{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9CXQySpjCm1"
   },
   "source": [
    "### Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10182,
     "status": "ok",
     "timestamp": 1753865920201,
     "user": {
      "displayName": "Salvatore Galati",
      "userId": "00612862714766166306"
     },
     "user_tz": -120
    },
    "id": "V92vHxErRdIw",
    "outputId": "6ab9a1e0-3910-4d79-8fd4-57c476dd13cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
      "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain faiss-cpu sentence-transformers pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 933,
     "status": "ok",
     "timestamp": 1753865921138,
     "user": {
      "displayName": "Salvatore Galati",
      "userId": "00612862714766166306"
     },
     "user_tz": -120
    },
    "id": "4R0tvBOCRBMt"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os, re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5PqyJwFggqu"
   },
   "source": [
    "### Section extraction and normalization from PDFs\n",
    "\n",
    "This cell defines regular expressions to detect common section titles in scientific papers (e.g., *Introduction*, *Methods*, *Results*, etc.), even when preceded by numbering (e.g., \"1. Introduction\").  \n",
    "The `normalize_section_title` function is used to **standardize these titles**, mapping variations to canonical categories (e.g., \"1. Introduction\", \"Background\" ‚Üí \"introduction\").  \n",
    "This helps to semantically organize and index the content more effectively in the downstream RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1753865921140,
     "user": {
      "displayName": "Salvatore Galati",
      "userId": "00612862714766166306"
     },
     "user_tz": -120
    },
    "id": "tH_-63o9WX7I"
   },
   "outputs": [],
   "source": [
    "pdf_dir = \"../data/Publications\"\n",
    "\n",
    "section_patterns = [\n",
    "    r\"(?:^\\d+[\\.\\)]?\\s*)?abstract\",\n",
    "    r\"(?:^\\d+[\\.\\)]?\\s*)?introduction\",\n",
    "    r\"(?:^\\d+[\\.\\)]?\\s*)?background\",\n",
    "    r\"(?:^\\d+[\\.\\)]?\\s*)?materials and methods\",\n",
    "    r\"(?:^\\d+[\\.\\)]?\\s*)?methods?\",\n",
    "    r\"(?:^\\d+[\\.\\)]?\\s*)?results\",\n",
    "    r\"(?:^\\d+[\\.\\)]?\\s*)?results and discussion\",\n",
    "    r\"(?:^\\d+[\\.\\)]?\\s*)?discussion\",\n",
    "    r\"(?:^\\d+[\\.\\)]?\\s*)?conclusions?\",\n",
    "    r\"(?:^\\d+[\\.\\)]?\\s*)?references\"\n",
    "]\n",
    "section_regex = re.compile(\"|\".join(section_patterns), re.IGNORECASE)\n",
    "\n",
    "def normalize_section_title(raw_title: str) -> str:\n",
    "    text = raw_title.lower().strip()\n",
    "\n",
    "    # Rimuove numbers/dots (example. \"1. \", \"2)\", \"3.1 \")\n",
    "    text = re.sub(r\"^\\d+(\\.\\d+)?[\\.\\)]?\\s*\", \"\", text)\n",
    "\n",
    "    # Explicit mapping\n",
    "    if \"abstract\" in text:\n",
    "        return \"abstract\"\n",
    "    elif \"introduction\" in text:\n",
    "        return \"introduction\"\n",
    "    elif \"background\" in text:\n",
    "        return \"introduction\"\n",
    "    elif \"material\" in text or \"method\" in text:\n",
    "        return \"materials and methods\"\n",
    "    elif \"result\" in text or \"discussion\" in text:\n",
    "        return \"results and discussion\"\n",
    "    elif \"conclusion\" in text:\n",
    "        return \"conclusion\"\n",
    "    elif \"reference\" in text:\n",
    "        return \"references\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tV3CFmYRhTZz"
   },
   "source": [
    "### Section-wise parsing of scientific PDFs\n",
    "\n",
    "Loops through all PDF files in the specified folder and uses `PyMuPDFLoader` to extract text page by page.  \n",
    "It then scans each line for section headers (like \"1. Introduction\", \"Results and Discussion\", etc.) using the previously defined regular expressions.\n",
    "\n",
    "When a section header is detected:\n",
    "- The content collected so far is saved as a `Document` object with metadata (`source` and `section`).\n",
    "- The parser starts buffering the new section's content.\n",
    "\n",
    "At the end of each file, the final section is saved as well.  \n",
    "The result is a list of semantically segmented sections from your papers ‚Äî stored in `sectioned_docs` ‚Äî ready for chunking and indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1028,
     "status": "ok",
     "timestamp": 1753865922169,
     "user": {
      "displayName": "Salvatore Galati",
      "userId": "00612862714766166306"
     },
     "user_tz": -120
    },
    "id": "aWQAIAsJTdWe",
    "outputId": "82d7b0aa-2865-463e-f410-19a697677cd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 8.Resveratrol Analogues as Dual Inhibitors of Monoamine Oxidase B and Carbonic Anhydrase VII A New Multi-Target Combination for Neurodegenerative Diseases.pdf\n",
      "Section found: 1. introduction Section mapping: introduction\n",
      "Section found: 2. results and discussion Section mapping: results and discussion\n",
      "Section found: 3. materials and methods Section mapping: materials and methods\n",
      "Section found: 4. conclusions Section mapping: conclusion\n",
      "Section found: references Section mapping: references\n",
      "File: 1.Development of a cheminformatics platform for selectivity analyses of carbonic anhydrase inhibitors.pdf\n",
      "Section found: abstract Section mapping: abstract\n",
      "Section found: introduction Section mapping: introduction\n",
      "Section found: materials and methods Section mapping: materials and methods\n",
      "Section found: results and discussion Section mapping: results and discussion\n",
      "Section found: conclusions Section mapping: conclusion\n",
      "Section found: references Section mapping: references\n",
      "File: 7.Machine Learning-Based Virtual Screening for the Identification of Cdk5 Inhibitors.pdf\n",
      "Section found: 1. introduction Section mapping: introduction\n",
      "Section found: 2. results and discussion Section mapping: results and discussion\n",
      "Section found: 3. materials and methods Section mapping: materials and methods\n",
      "Section found: 4. conclusions Section mapping: conclusion\n",
      "Section found: references Section mapping: references\n",
      "File: 5..VenomPred A Machine Learning Based Platform for Molecular Toxicity Predictions.pdf\n",
      "Section found: 1. introduction Section mapping: introduction\n",
      "Section found: 2. materials and methods Section mapping: materials and methods\n",
      "Section found: 3. results and discussion Section mapping: results and discussion\n",
      "Section found: 4. conclusions Section mapping: conclusion\n",
      "Section found: references Section mapping: references\n",
      "File: 6.Identification of Human Dihydroorotate Dehydrogenase Inhibitor by a Pharmacophore-Based Virtual Screening Study.pdf\n",
      "Section found: 1. introduction Section mapping: introduction\n",
      "Section found: 2. materials and methods Section mapping: materials and methods\n",
      "Section found: 3. results and discussion Section mapping: results and discussion\n",
      "Section found: 4. conclusions Section mapping: conclusion\n",
      "Section found: references Section mapping: references\n",
      "File: 3.Predicting Isoform-Selective Carbonic Anhydrase Inhibitors via Machine Learning and Rationalizing Structural Features Important for Selectivity.pdf\n",
      "Section found: 1. introduction Section mapping: introduction\n",
      "Section found: 2. materials and methods Section mapping: materials and methods\n",
      "Section found: 3. results and discussion Section mapping: results and discussion\n",
      "File: 2.Discovery of a new ATP citrate lyase ACLY inhibitor identified by a pharmacophore based virtual screening study.pdf\n",
      "Section found: abstract Section mapping: abstract\n",
      "Section found: introduction Section mapping: introduction\n",
      "Section found: results and discussion Section mapping: results and discussion\n",
      "Section found: conclusion Section mapping: conclusion\n",
      "Section found: materials and methods Section mapping: materials and methods\n",
      "Section found: method Section mapping: materials and methods\n",
      "Section found: references Section mapping: references\n",
      "Extracted section: 36\n"
     ]
    }
   ],
   "source": [
    "sectioned_docs = []\n",
    "\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    if not filename.endswith(\".pdf\"):\n",
    "        continue\n",
    "\n",
    "    print(\"File:\",filename)\n",
    "    loader = PyMuPDFLoader(os.path.join(pdf_dir, filename))\n",
    "    pages = loader.load()\n",
    "    #break\n",
    "\n",
    "    current_section = None\n",
    "    buffer = \"\"\n",
    "\n",
    "    for page in pages:\n",
    "        text = page.page_content.strip()\n",
    "        lines = text.split(\"\\n\")\n",
    "\n",
    "        for line in lines:\n",
    "            clean = line.strip().lower()\n",
    "            section_title = normalize_section_title(clean)\n",
    "\n",
    "            # Check section title\n",
    "            if re.fullmatch(section_regex, clean):\n",
    "                print(\"Section found:\", clean, \"Section mapping:\",section_title)\n",
    "                # If there is an open section, save it\n",
    "                if current_section:\n",
    "                    sectioned_docs.append(Document(\n",
    "                        page_content=buffer.strip(),\n",
    "                        metadata={\"source\": filename, \"section\": section_title}\n",
    "                    ))\n",
    "                # Start new section\n",
    "                current_section = section_title\n",
    "                buffer = \"\"\n",
    "            else:\n",
    "                buffer += line + \"\\n\"\n",
    "\n",
    "    # Last section at the end of the file\n",
    "    if current_section and buffer:\n",
    "        sectioned_docs.append(Document(\n",
    "            page_content=buffer.strip(),\n",
    "            metadata={\"source\": filename, \"section\": current_section}\n",
    "        ))\n",
    "\n",
    "print(f\"Extracted section: {len(sectioned_docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6t7_-REhwwd"
   },
   "source": [
    "### Chunking, embedding, and FAISS index creation\n",
    "\n",
    "üì¶ Final output: a serialized FAISS vector index saved locally ready to support RAG-based querying.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6U_-l3eaiFue"
   },
   "source": [
    "**Text Chunking**  \n",
    "   It uses `RecursiveCharacterTextSplitter` to break down each section (from `documents`) into manageable chunks of 1000 characters, with 200-character overlap to preserve context across boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1753865934645,
     "user": {
      "displayName": "Salvatore Galati",
      "userId": "00612862714766166306"
     },
     "user_tz": -120
    },
    "id": "pjMEx_24R7hC",
    "outputId": "324de158-5c6f-4483-d360-c3e3951a5e11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Documents splitted in 465 chunk\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "docs = text_splitter.split_documents(sectioned_docs)\n",
    "print(f\"üìÑ Documents splitted in {len(docs)} chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1850,
     "status": "ok",
     "timestamp": 1753865939827,
     "user": {
      "displayName": "Salvatore Galati",
      "userId": "00612862714766166306"
     },
     "user_tz": -120
    },
    "id": "dp91lCbve1gU",
    "outputId": "3b03fdb0-8cf7-4b08-cc0f-fe7d58198402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? True\n"
     ]
    }
   ],
   "source": [
    "#Verifies if a GPU is available using PyTorch, which helps speed up embedding when supported.\n",
    "import torch\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBWCz0AGiUGM"
   },
   "source": [
    "**Embedding Model Selection**  \n",
    "   Three embedding models are suggested:\n",
    "   - `MiniLM-L6-v2`: fast and lightweight, great for testing\n",
    "   - `BAAI/bge-small-en-v1.5`: optimized for semantic search and QA tasks (recommended for most use cases)\n",
    "   - `intfloat/e5-large-v2`: larger and more powerful, ideal for RAG production\n",
    "\n",
    "   In this setup, the **BGE-small model** is used, with normalized embeddings and loaded on GPU (`device=\"cuda\"`).\n",
    "\n",
    "**FAISS Index Construction**  \n",
    "   The chunks are embedded and indexed using FAISS ‚Äî a fast similarity search library.  \n",
    "   The resulting `db` object allows for efficient semantic retrieval later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4969,
     "status": "ok",
     "timestamp": 1753865976245,
     "user": {
      "displayName": "Salvatore Galati",
      "userId": "00612862714766166306"
     },
     "user_tz": -120
    },
    "id": "78-j2bDYSAgh",
    "outputId": "78e92f54-24d0-4988-d665-ea6a78d41575"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç FAISS index saved in '/content/faiss_index'\n"
     ]
    }
   ],
   "source": [
    "VECTOR_PATH = \"../vectorstore\"\n",
    "\n",
    "#For fast tests\n",
    "#embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#For Q/A\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\", encode_kwargs={\"normalize_embeddings\": True}, model_kwargs={\"device\": \"cuda\"})\n",
    "\n",
    "#For RAG production\n",
    "#embedding = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large-v2\", encode_kwargs={\"normalize_embeddings\": True})\n",
    "\n",
    "#FAISS\n",
    "db = FAISS.from_documents(docs, embedding)\n",
    "\n",
    "#Save the FAISS index lovally for later use\n",
    "db.save_local(VECTOR_PATH)\n",
    "print(f\"üîç FAISS index saved in '{VECTOR_PATH}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYvyKtBGjuVa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNIA3hdqSX/4G9dVYi2iSWL",
   "gpuType": "T4",
   "mount_file_id": "1dTEtnX0PLIRKfc_kGL64_hfe4vzZ6t6Z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
